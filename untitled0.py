# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OqcLjKpjv2JalNEic6X92iWKADiJUJJ9
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch.autograd import Variable
import torch.nn as nn
import random
import torch.nn.functional as F
import torch.optim as optim
import nltk
nltk.download('abc')
nltk.download('punkt')
from nltk.corpus import abc
import string
import numpy as np
from torch.utils.data import DataLoader
from torch.utils.data import Dataset, DataLoader
from datetime import datetime
import pickle as pkl
from sklearn.manifold import TSNE
# %matplotlib inline
import matplotlib.pyplot as plt
import pickle as pkl

torch.manual_seed(1)

CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right
text = abc.raw().lower().split()
text2 = []
for i in text:
  word = ''
  for j in i:
    if j not in string.punctuation:
      word+=j
  if word != '':
    text2.append(word)
# text = [''.join(c for c in s if c not in string.punctuation) for s in text]
# text = [s for s in text if s]

text = text2

vocab = set(text)
vocab_size = len(vocab)
print('vocab_size:', vocab_size)

w2i = {w: i for i, w in enumerate(vocab)}
i2w = {i: w for i, w in enumerate(vocab)}

print(w2i[text[0]])

def create_skipgram_dataset(text):
    data = []
    for i in range(2, len(text) - 2):
        data.append((w2i[text[i]], w2i[text[i-2]], 1))
        data.append((w2i[text[i]], w2i[text[i-1]], 1))
        data.append((w2i[text[i]], w2i[text[i+1]], 1))
        data.append((w2i[text[i]], w2i[text[i+2]], 1))
        # 4 times since each word has 4 context words
        idxes = []
        for j in range(4):
          idx = random.randint(0,len(text)-1)
          while(idx in range(i-2,i+3) or idx in idxes):
            idx = random.randint(0,len(text)-1)
          idxes.append(idx)
          data.append((w2i[text[i]], w2i[text[idx]], 0))
    return data

skipgram_train = create_skipgram_dataset(text)

skipgram_train = np.array(skipgram_train)

batch_size = 2832
class create_index_pairs(Dataset):
    def __init__(self, focus_context_idx, w2i, i2w):
        self.focus_context_idx = focus_context_idx
        self.w2i = w2i
        self.i2w = i2w
        
    def __len__(self):
        return len(self.focus_context_idx)
    
    def __getitem__(self, index):
        return self.focus_context_idx[index]

dataset = create_index_pairs(skipgram_train,w2i,i2w)

dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class SkipGram(nn.Module):
    def __init__(self, inp_size, hidden_size,batch_size):
        super(SkipGram, self).__init__()
        self.batch_size = batch_size
        self.embeddings = nn.Embedding(inp_size, hidden_size)
    
    def forward(self, focus, context):
        embed_focus = self.embeddings(focus).view((self.batch_size,1,-1))
        embed_ctx = self.embeddings(context).view((self.batch_size,-1,1))
        pred = torch.bmm(embed_focus, embed_ctx)
        pred = F.logsigmoid(pred)
    
        return pred[:,0,0]

embd_size = 100
learning_rate = 0.005
n_epoch = 5
import pickle as pkl

def train_skipgram():
    losses = []
    MSELoss = nn.MSELoss()
    model = SkipGram(vocab_size, embd_size,batch_size).to(device)
    print(model)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    for epoch in range(n_epoch):
        total_loss = .0
        start_time = datetime.now()
        for count, data in enumerate(dataloader):
          optimizer.zero_grad()
          focus = data[:,0].to(device)
          context = data[:,1].to(device)
          target = data[:,2].to(device)
          pred = model(focus,context)
          loss = MSELoss(pred, target.float())
          loss.backward()
          optimizer.step()
          total_loss+=loss.data
          print("\r batch:{}/{} \t time:{} \t loss:{} \t total_loss:{}".format(count+1, len(dataloader), datetime.now()-start_time,loss.data,total_loss/len(dataloader)), end="")
        print("")
        print("Epoch Loss: {}".format(total_loss/len(dataloader)))
        embedding = model.embeddings.weight.data.cpu()
        tsne = TSNE(n_components = 2,init = 'pca',verbose =1,random_state = 21).fit_transform(embedding)
        plt.figure()
        x = tsne[:,0]
        y = tsne[:,1]
        plt.scatter(x,y)
        name = 'w2v'+str(epoch)+'.png'
        plt.savefig(name)
    return model

sg_model = train_skipgram()

pkl.dump(sg_model,open('sg_model','wb'))

sg_model = pkl.load(open('sg_model','rb'))

embedding = sg_model.embeddings.weight.data.cpu()

tsne = TSNE(n_components = 2,init = 'pca',verbose =2,random_state = 21).fit_transform(embedding)

for count,i in enumerate(vocab):
  idx = w2i[i]
  print(count,i,tsne[idx][0],tsne[idx][1])

  plt.scatter(tsne[idx][0],tsne[idx][1])
  plt.annotate(i, xy = (tsne[idx][0], tsne[idx][1]), ha='right',va='bottom')
plt.savefig("w2v.png")
plt.show()

